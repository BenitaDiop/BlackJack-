{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "581740d4-e46b-428e-8352-ab177187b583"
    }
   },
   "source": [
    "![toc](https://github.com/BenitaDiop/students/blob/master/bin/Cables%20(1).png)\n",
    "\n",
    "\n",
    "\n",
    "# I.  [Abstract](#abstract) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# II. [Multivariate Regression Analysis](#regression)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# III. [Discriminant and Classification Analysis on Student Data](#discrim) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# IV. [Principal Component Analysis  on Student Data](#prin)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# V. [Factor Analysis on Student Data](#fact)\n",
    "\n",
    "\n",
    "\n",
    "# VII. [Bibliography ](#ref)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# VIII. [Addendum](#add)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "![Cabl](https://raw.githubusercontent.com/BenitaDiop/students/master/bin/Cables.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "c4eb6f8b-30b6-446c-adc1-1d524751380a"
    }
   },
   "source": [
    "<a id=\"abstract\"></a>\n",
    "# ABSTRACT \n",
    "\n",
    "\n",
    "Education has been widely  denoted as one of the key factors for long-term economic  upward mobility.  The main objective of this project is to perform an extensive high-level explanatory analysis on student achievement data using a variety of robust multivariate statistical analysis techniques. The real-world data used in this project is a collection of school reports and questionnaires around the many factors  surrounding portugues student success/failure rates in two core classes: Mathematics and Language:Portuguese..  The reason that multivariate statistical analysis on  student data is of vital importance is because if models with high predictive powers are developed they provide an opportunity to both improve the quality of education in students and enhance the management of academic resources in institutions. Analysis of student data has the opportunity of asking a multitude of questions such as : \n",
    "\n",
    "- Who are the students taking most credit hours \n",
    "- Who is likely to return for more classes?\n",
    "- What type of courses can be offered to attract more students? \n",
    "- What are the main reasons for student transfers?\n",
    "- Is it possible to predict student performance? \n",
    "- What are the factors that affect student achievement?\n",
    "\n",
    "\n",
    "In this project in order to predict student achievement and possibly identify the key variables that affect educational success/failure multivariate statistical methods that will be used are: multivariate regression analysis., discriminant and classification analysis, principal component analysis, factor analysis and cluster analysis. During the explanatory analysis of the data, once the best model has been identified the goal is to identify the most relevant features that influence academic success /failure. Table 1 shows the variables used in the analysis. To get a clear visual of the dispersion and central tendency of each variable within the dataset a univariate analysis of the central tendency and dispersive behavior of each variable is shown in figure 1. \n",
    " \n",
    "\n",
    "![var.png](https://github.com/BenitaDiop/students/blob/master/bin/var.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "c4eb6f8b-30b6-446c-adc1-1d524751380a"
    }
   },
   "source": [
    "![tabl.png](https://raw.githubusercontent.com/BenitaDiop/students/master/bin/tabl.png)\n",
    "\n",
    "The population variance $\\sigma _{j}^{2}$ can be estimated by the sample variance\n",
    "$$\\begin{align} s_j^2 &= \\frac{1}{n-1}\\sum_{i=1}^{n}(X_{ij}-\\bar{x}_j)^2\\\\&= \\frac{\\sum_{i=1}^{n}X_{ij}^2- n \\bar{x}_j^2 }{n-1} \\\\&=\\frac{\\sum_{i=1}^{n}X_{ij}^2-\\left(\\left(\\sum_{i=1}^{n}X_{ij}\\right)^2/n\\right)}{n-1} \\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "c4eb6f8b-30b6-446c-adc1-1d524751380a"
    }
   },
   "source": [
    "<a id=\"regression\"></a>\n",
    "# Multivariate Regression Analysis\n",
    "Multivariate multiple regression is a technique where multiple response values are measured on multiple predictor variables. The objective of this method is to relate our responses to our predictors, which will help us with estimation and testing parameters included in the model. The model includes an $(n x p) Y$ matrix, $n x (q + 1) X$ matrix and a $(q + 1) x p B$ matrix.\n",
    "\n",
    "If we actually let i = 1, ..., n, we see that we obtain n equations:\n",
    "$$\\begin{align} \n",
    "y_1 & =\\beta_0+\\beta_1x_1+\\epsilon_1 \\\\ \n",
    "y_2 & =\\beta_0+\\beta_1x_2+\\epsilon_2 \\\\ \n",
    "\\vdots \\\\ \n",
    "y_n & = \\beta_0+\\beta_1x_n+\\epsilon_n \n",
    "\\end{align}$$\n",
    "We can instead formulate the above simple linear regression function in matrix notation:\n",
    "\n",
    "$$\\underbrace{\\vphantom{\\begin{bmatrix} \n",
    "1 & x_1\\\\ \n",
    "1 & x_2\\\\ \n",
    "\\vdots &\\vdots\\\\1&x_n  \\end{bmatrix}}\n",
    "\\begin{bmatrix} \n",
    " y_1\\\\ \n",
    " y_2\\\\ \n",
    "\\vdots\\\\y_n \n",
    "\\end{bmatrix}}_{\\textstyle \n",
    "\\begin{gathered}Y\\end{gathered}}=\\underbrace{\\begin{bmatrix} \n",
    "1 & x_1\\\\ \n",
    "1 & x_2\\\\ \n",
    "\\vdots &\\vdots\\\\1&x_n \n",
    "\\end{bmatrix}}_{\\textstyle \n",
    "\\begin{gathered}=X\\end{gathered}} \\underbrace{\\vphantom{\\begin{bmatrix} \n",
    "1 & x_1\\\\ \n",
    "1 & x_2\\\\ \n",
    "\\vdots &\\vdots\\\\1&x_n \n",
    "\\end{bmatrix}}\\begin{bmatrix} \n",
    "\\beta_0 \\\\ \n",
    "\\beta_1\\\\ \n",
    "\\end{bmatrix}}_{\\textstyle \n",
    "\\begin{gathered}\\beta\\end{gathered}}+\\underbrace{\\vphantom{\\begin{bmatrix} \n",
    "1 & x_1\\\\ \n",
    "1 & x_2\\\\ \n",
    "\\vdots &\\vdots\\\\1&x_n \n",
    "\\end{bmatrix}}\\begin{bmatrix}\\epsilon_1\\\\\\epsilon_2\\\\\\vdots\\\\\\epsilon_n \n",
    "\\end{bmatrix}}_{\\textstyle \n",
    "\\begin{gathered}+\\epsilon\\end{gathered}}$$\n",
    "\n",
    "\n",
    "\n",
    "### ASSUMPTIONS\n",
    "- The mean of the response, $E(Yi)$, at each set of values of the predictors, $(x1i,x2i,...)$, is a Linear function of the predictors.\n",
    "- The errors, $ϵi$, are Independent.\n",
    "- The errors, $ϵi$, at each set of values of the predictors, $(x1i,x2i,...)$, are Normally distributed.\n",
    "- The errors, $ϵi$, at each set of values of the predictors, $(x1i,x2i,...)$, have Equal variances (denoted $α2$).\n",
    "\n",
    "\n",
    "### LEAST SQUARES ESTIMATION\n",
    "$$b=\\begin{bmatrix} \n",
    "b_0\\\\ \n",
    "b_1\\\\ \n",
    "\\vdots\\\\ \n",
    "b_{p-1} \n",
    "\\end{bmatrix}=(X^{'}X)^{-1}X^{'}Y \\;   \\;   \\;   \\;   \\;     $$\n",
    "\n",
    "\n",
    "$$\\vdots$$\n",
    "$$X^{'}X=\\begin{bmatrix} \n",
    "n & \\sum_{i=1}^{n}x_i \\\\ \n",
    "\\sum_{i=1}^{n}x_i  & \\sum_{i=1}^{n}x_{i}^{2} \n",
    "\\end{bmatrix}\\;   \\;    \\;   \\;   \\; $$\n",
    "\n",
    "\n",
    "\n",
    "**Dataset Specs:**\n",
    "$n = 649$\n",
    "$\\;   \\;   \\; p = 3$\n",
    "$\\; \\; \\; \\; \\;  q = 12$\n",
    "\n",
    "### Test of Overall Regression\n",
    "In order to determine if the model is a good fit for our data we can run a test of overall regression. For this test we assume that $yi ~ Np(x_i\\beta, \\Sigma)\\;$\n",
    "\n",
    "$H_0: B_1 = 0 \\; \\; vs. \\; \\;H_a: B_1 ≠ 0 $\n",
    "\n",
    "From our model we know:\n",
    "$p = 3, q = 12, n = 649$. From that we compute $V_h = 12, V_E = 649 – 12 – 1 = 636, s = 3, M = 4, N = 316$\n",
    "\n",
    "**Wilk**=0.6929, The critical value $\\Lambda_{(.05, 3, 12, 636)} = 0.919 > 0.6929$, so we reject H0.\n",
    "\n",
    "**Roy's Maximum Root: Largest eigenvalue of $HE^{-1)$**\n",
    "$\\theta= \\lambda_1/(1+ \\lambda_1 )=0.2769$\n",
    "θ (.05, 3, .4, 316) = 0.058 < 0.2769, so we reject H0.\n",
    "\n",
    "**Pillai**=0.3191\n",
    "\n",
    "$V_(s)(.05, 3, 4, 316) = 0.740 > 0.3191$, so we do not reject H0. Table for Pillai only goes to 25, so this is most likely not accurate. \n",
    "\n",
    "**Lawley-Hotelling Test***=0.4261\n",
    "$U(s)(.05, 3, 12, 636) = 4.1104 < 636/12 = 30.25$, so we reject H0.\n",
    "\n",
    "Because our data set is so large, a few of these critical values are not accurate. In that case, we can use the exact p-value provided by SAS. As we can see all tests are considered significant meaning that there is linear association between our response variables G1, G2 , and G3 and our predictor variables.\n",
    "\n",
    "### Test on a Subset of the x’s\n",
    "When looking at the p-values of our coefficients, a few stand out as being insignificant in the model. To test if this is accurate, we can test significance on subset of those x’s to see if the y’s depend on them or not. Below is a test on a subset of the variables Dalc, Walc, and goout to see if the y’s depend on them.\n",
    "$H_0: B_d = 0 \\; \\; vs. \\; \\;H_a: B_d ≠ 0 $\n",
    "\n",
    "$$\\text{*Wilks Lambda}\\Lambda^* = \\dfrac{|\\mathbf{E}|}{|\\mathbf{H+E}|}$$\n",
    "\n",
    "**Wilks=0.9765,  The critical value $\\Lambda_{(.05, 3, 12, 636)} = 0.919 < 0.9765 $, so we do not reject H0**. Like previously mentioned, the critical values for the test statistics may not be completely accurate because of the size of our dataset, so we can use the exact p-values from the SAS output to determine significance. We can see that only Roy’s greatest root is significant, so we can conclude that we these are not significant predictor’s given the x’s already in the model.\n",
    "\n",
    "![REGR.png](https://github.com/BenitaDiop/students/blob/master/bin/REGR.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"discrim\"></a>\n",
    "# Discriminant and Classification Analysis\n",
    "\n",
    "There are two major objectives in group separation:\n",
    "> Description of group separation in which linear functions of the variables are used to describe the difference between the groups. The goals of descriptive discriminant analysis are to identify the relative contribution of the variables to separation of the groups. As well as, finding the optimal plane on which the point can be projected to best illustrate the configuration.\n",
    "\n",
    "> Prediction allocation of observations to groups, in which linear or quadratic functions of the variables are employed to assign an individual sampling unit to one of the groups. The measured values in the observation vector for an individual or object are evaluated by the classification function to find the group to which the individual most likely belongs. \n",
    "\n",
    "\n",
    "### Disciminant Analysis \n",
    "We use discriminant analysis for objective 1 and classification analysis for objective 2. From this data set numerical values were used for discriminant and classification analysis. For this analysis the variables were grouped by study time, group 1 <2 hours, group 2 2 to 5 hours, group 3 was 5-10 hours and group 4 is >10 hours. In discriminant analysis for several groups, we want to find a linear combination of variables that best separate the groups of multivariate observations. Discriminant analysis for several groups may serve any one of various purpose:\n",
    "- Examine group separation in a two-dimensional plot. When there are more than two groups, it requires more than one discriminant function to describe group separation. If the points in the p-dimensional space are projected onto a two-dimensional space represented by the first discriminant functions, we obtain the best possible view of how the groups are separated.\n",
    "- Find a subset of the original variables that separates the group almost as well as the original set\n",
    "- Rank the variables in term of their relative contribution to group separation. \n",
    "- Interpret the new dimensions represent by the discriminant functions\n",
    "- Follow up to fixed-effect MANOVA\n",
    "\n",
    "\n",
    "\n",
    "**Linear discriminant analysis is for homogeneous variance-covariance matrices:**\n",
    "$$\\Sigma_1 = \\Sigma_2 = \\dots = \\Sigma_g = \\Sigma$$\n",
    "In this case the variance-covariance matrix does not depend on the population. <br> \n",
    "> Linear Discriminant Function $d^L_i(\\mathbf{x}) = -\\dfrac{1}{2}\\mathbf{\\mu'_i\\Sigma^{-1}\\mu_i + \\mu'_i\\Sigma^{-1}x} = d_{i0} + \\sum_{j=1}^{p}d_{ij}x_j$ <br> \n",
    "Linear Score Function $s^L_i(\\mathbf{X}) = -\\dfrac{1}{2}\\mathbf{\\mu'_i \\Sigma^{-1}\\mu_i + \\mu'_i \\Sigma^{-1}x}+ \\log p_i = d_{i0}+\\sum_{j=1}^{p}d_{ij}x_j + \\log p_i$\n",
    "\n",
    "\n",
    "**Quadratic discriminant analysis is used for heterogeneous variance-covariance matrices:**\n",
    "$\\Sigma_i \\ne \\Sigma_j$ for some $i \\ne j$ \n",
    "This allows the variance-covariance matrices to depend on the population.\n",
    "> Quadratic Score: $$\\hat{s}^Q_i (\\mathbf{x}) = -\\frac{1}{2}\\log{|\\mathbf{S_i}|}-\\frac{1}{2}{\\mathbf{(x-\\bar{x}_i)'S^{-1}_i(x - \\bar{x}_i)}}+\\log{p_i}$$\n",
    "\n",
    "We are testing to determine if the variance-covariance matrices are homogeneous for all populations involved.The eigenvector of E-1H, which is also the vectors of the discriminant function coefficients, table 2.1 \n",
    "The standardized discriminant function coefficients are given in table 2.2. The eigenvalues are 0.1507, 0.0414 and 0.0274, the first eigenvalue accounts for a large portion of the total shown in table 2.3. The first discriminant function is more significant than the second and third one.Since our data set is so large, the critical values could not be determined from the table provided by the book since the table only goes to a p value of 8 while our data has a p value of 15. Therefore, the exact p values produced by SAS are provided above. We can see that all tests are significant meaning they are significantly separated. \n",
    "\n",
    "\n",
    "### Classification Analysis \n",
    "Classification analysis is the allocation of observations to groups, which is the predictive aspect of discriminant analysis. In classification, a sampling unit who group membership is unknown is assigned to a group on basis of the vector of p measured values, y, associated with the unit. \n",
    "\n",
    "Classification into several groups, depends on whether or not the covariance matrices are equal or not. We use a sample from each of the k groups to find the sample mean vector. For a vector whose group membership is unknown, in approach is to use a distance function to find the mean vector that y is closest to and assign y to the corresponding group. Let Si denote the sample variance-covariance matrix for population i.\n",
    "\n",
    "$$\\mathbf{S}_p = \\dfrac{\\sum_{i=1}^{g}(n_i-1)\\mathbf{S}_i}{\\sum_{i=1}^{g}(n_i-1)}$$\n",
    "***For populations with equal covariance matrices we use linear classification function ***\n",
    "\n",
    "$$D_2(y)=(y−\\bar{y})'S^{−1}y−\\bar{y})$$\n",
    "\n",
    "***For populations with unequal covariance matrices we use the quadratic classification function***\n",
    "$$D_2(y) = (y − \\bar{y} )′S^{−1}(y − \\bar{y} ), i = 1, 2, . . . , k,$$\n",
    "\n",
    "\n",
    "To understand the capability of classification procedures to predict group membership, we use the bayes rule, probability of misclassification, which is known as the error rate.$\\; \\; P(\\text{member of } \\pi_i | \\text{ we observed } \\mathbf{x}) = \\dfrac{P(\\text{member of } \\pi_i \\text{ and we observe } \\mathbf{x})}{P(\\text{we observe } \\mathbf{x})} \\; \\;$ A simple estimate of the error rate can be obtained by trying the classification procedure on the same data set that has been used to compute the classification functions. This method is commonly referred to as resubsutition. Each observation vector is summited to the classification function and assigned to a group. We then count the number of correct classification results from resubsitution is called the apparent error rate. Apparent error rate = 1 − apparent correct classification rate and also Apparent error rate=  $\\frac{n_12+n_2}{n_1+n_2}$\n",
    "\n",
    "##### k nearest neighbor classification rule\n",
    "For the k nearest neighbor classification rule. We compute the distance from an observation yi to all other points yj using the distance function $(yi −yj)′S−1(yi −yj), j ̸=i$ Classify yi into one of two groups, the k points nearest to yi are examined, and if the majority of the k points belong to G1, assign yi to G1; otherwise assign yi to G2.When coming the linear classification method to the quadratic classification method for several groups. Linear classification method is used for covariance matrices that are equal while quadratic classification is used for unequal covariance matrices. In this class I conducted both since the equality of the matrices was unknown. The results below show that quadratic was the best method since is produced a much smaller error rate compared to that of linear classification. The group with the most difficulty to group is group 1 there is a large amount of  variation since that is the study time with the lowest time.  A k value of 5 was selected for nearest neighbors because it produced the smallest error values compared other values of k that we’re tested in the system. The error rate is 0.4039. \n",
    "\n",
    "![DIS](https://raw.githubusercontent.com/BenitaDiop/students/master/bin/DIS.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"pric\"></a>\n",
    "# Principal Component Analysis \n",
    "\n",
    "\n",
    "Many real world data often times containe a very large number of explanatory variables and its becomes difficult trying to extract the most important ones.  Principal Component Analysis, an eigenvector-based multivariate dimensionality-reducntion analyses, is an operation that reveals the internal structure of a given dataset in a way that best explains the variance. A good practice is to turn the variables into principal compomnents before attempting further analysis on the data. In a multidimensional space, attaining the best fitting line and additional best fitting line(s) perpendicular to the first yields an orthogonal basis. These basis vectors where different individual dimensions are uncorrelated are called principal components. \n",
    "\n",
    "Principal component analysis is very similar to another anallysis that we will use later, factor analysis. In principal component each component is viewed as *a weighted combination of the input variables* with as many components as they are variables.\n",
    "\n",
    "\n",
    "Given a random  vector \n",
    "$$ \\textbf{X} = \\left(\\begin{array}{c} X_1\\\\ X_2\\\\ \\vdots \\\\X_p\\end{array}\\right) $$\n",
    "With a given population variance-covariance matrix\n",
    "$$ \\text{var}(\\textbf{X}) = \\Sigma = \\left(\\begin{array}{cccc}\\sigma^2_1 & \\sigma_{12} & \\dots &\\sigma_{1p}\\\\ \\sigma_{21} & \\sigma^2_2 & \\dots &\\sigma_{2p}\\\\  \\vdots & \\vdots & \\ddots & \\vdots \\\\ \\sigma_{p1} & \\sigma_{p2} & \\dots & \\sigma^2_p\\end{array}\\right)$$\n",
    "We consider the linear combinations\n",
    "$$\\begin{array}{lll} Y_1 & = & e_{11}X_1 + e_{12}X_2 + \\dots + e_{1p}X_p \\\\ Y_2 & = & e_{21}X_1 + e_{22}X_2 + \\dots + e_{2p}X_p \\\\ & & \\vdots \\\\ Y_p & = & e_{p1}X_1 + e_{p2}X_2 + \\dots +e_{pp}X_p\\end{array}  $$\n",
    "\n",
    "\n",
    "We select  $\\boldsymbol { e } _ { i1 , } \\boldsymbol { e } _ { i2 } , \\ldots , \\boldsymbol { e } _ { i p }$to maximize $ \\text{var}(Y_i) = \\sum\\limits_{k=1}^{p}\\sum\\limits_{l=1}^{p}e_{ik}e_{il}\\sigma_{kl} = \\mathbf{e}'_i\\Sigma\\mathbf{e}_i$ subject to the constraint that the sums of squared coefficients add up to one...along with the additional constraint that this new component is uncorrelated with all the previously defined components.\n",
    "$$ \\mathbf{e}'_i\\mathbf{e}_i = \\sum\\limits_{j=1}^{p}e^2_{ij} = 1$$\n",
    "$$\\text{cov}(Y_1, Y_i) = \\sum\\limits_{k=1}^{p}\\sum\\limits_{l=1}^{p}e_{1k}e_{il}\\sigma_{kl} = \\mathbf{e}'_1\\Sigma\\mathbf{e}_i = 0$$\n",
    "$$\\text{cov}(Y_2, Y_i) = \\sum\\limits_{k=1}^{p}\\sum\\limits_{l=1}^{p}e_{2k}e_{il}\\sigma_{kl} = \\mathbf{e}'_2\\Sigma\\mathbf{e}_i = 0$$\n",
    "$$\\text{cov}(Y_{i-1}, Y_i) = \\sum\\limits_{k=1}^{p}\\sum\\limits_{l=1}^{p}e_{i-1,k}e_{il}\\sigma_{kl} = \\mathbf{e}'_{i-1}\\Sigma\\mathbf{e}_i = 0$$\n",
    "\n",
    "\n",
    "In which each of the weighted combination of the input variables can be interpreted as coeffienct from an OLS model, where we are predicting $Y_{i}$ from $X_{1}$, ${X_2}$, ... ,$X_{n}$ . We do noot have a intercept, howeevr, $e_{i1}$,$e_{i2}$ , ...,  can be viewed as regression coefficients.Additionally, Principal components are similar to canonical correlation analysis; canonical correlations define a new coordinate system that finds the optimal cross variance between two datasets, while the principal component defines new orthogonal coordinate systems that optimally describe the variance in a single dataset.\n",
    "\n",
    "Principal componnent analysis is not a hypothesis test but moreso a desctiptive analysis preprocessing step to find a concise decscription of the data. The fnal test to determine the significance of a component turns into a decision made by the analyzer. \n",
    "**In principle component analysis, the decision on how many principal components should be retained in order to effectively summarize a data is determined by four methods**\n",
    "- Retaining a sufficient component that can account for a specified percentage of the total variance \n",
    "- Retaining components whose eigenvalues are greater than the average of the eigenvalues \n",
    "- Observing a scree graph and looking for a natural break between the large eigenvalues and small eigenvalues \n",
    "- Test for the significance of the larger eigenvalues \n",
    "\n",
    "One downfall of principal componenet analysis that it may lead to an ambigious interpretation in the analysis.Hence why we may use factor rotation in factor analysis to reduce the complexity and retrieve are more clean data\n",
    "\n",
    "\n",
    "![](https://raw.githubusercontent.com/BenitaDiop/students/master/bin/PCA.png)\n",
    "\n",
    "##### Student Dataset Pricnipal Component Interpretation\n",
    "Principal Component Analysis was done on the full dataset. Examing the signevalues from the SAS output and the proportion of variation explained we determined the we should consider the first two components which determine 86% of the variation within the data. Additionally comparing the subsequent difference between eigenvalues it can be seen that the diffference gets smalelr and smaller; Additionally  when evaluating the components with the scree plot the remining eigenvalues are all relatively small after the 2nd component.\n",
    "\n",
    "\n",
    "Deciding to use just the first principal componet, which account for 52% of the variation within the data. The first principal component is given \n",
    "$$Y_i = 0.028(age) + 0.013(Medu) + 0.0032 (Fedu) + -.001(traveltime) + -.0068(studytime) + .0062(failures) - .0071(freetime) + 0.0067(goout) + .0129(Dalc) + 0.0228(wWalc) -.0052(health) + .998(absence) - .0153(G1) + -.0173(G2) + .0176(G3) $$\n",
    "\n",
    "From this it is determined that out of all the variable student absence had the greatest magnitude; thus greatest impact of a students performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"fact\"></a>\n",
    "# Factor Analysis\n",
    "\n",
    "In factor analysis we wish to reduce the redundancy among our variables by using a smaller number of factors.\n",
    "If the variables $y_1, y_2, . . . , y_p$ are correlated, the basic dimensionality of the entire system is less than $p$.\n",
    "Like Principal Component Analysis, Factor Analysis is a technique for dimensionality reduction, but there are some key differences.\n",
    "\n",
    "- In Principal Component Analysis, the goal is to explain a large part of the total variance of the variables. In factor analysis, the covariances or correlations among the variables is accounted for.\n",
    "\n",
    "- Principal components are expressed as linear combinations of the *original variables*. In factor analysis, the original variables are defined as linear combinations of the *factors*.\n",
    "\n",
    "- The estimated factors change if the number of factors changes. This is not true for principal components.\n",
    "\n",
    "- Principal components are unique with the assumption that there are distinct eigenvalues of **S**. Factors are subject to arbitrary rotation.\n",
    "\n",
    "\n",
    "The ability to rotate factors gives us the advantage of interpretability. \n",
    "This is one of the reasons that one may choose Factor Analysis over PCA. \n",
    "It is useful for finding and describing some underlying factors. \n",
    "\n",
    "All numeric variables in this dataset will be used for factor analysis. These include:\n",
    "\n",
    "|Variable|Description|\n",
    "|--------|-----------|\n",
    "|age|student's age |\n",
    "|Medu|mother's education |\n",
    "|Fedu|father's education |\n",
    "|traveltime|home to school travel time |\n",
    "|studytime|weekly study time |\n",
    "|failures|number of past class failures |\n",
    "|famrel|quality of family relationships |\n",
    "|freetime|free time after school |\n",
    "|goout|going out with friends |\n",
    "|Dalc|workday alcohol consumption |\n",
    "|Walc|weekend alcohol consumption |\n",
    "|health|current health status |\n",
    "|absences|number of school absences |\n",
    "|G1|first period grade |\n",
    "|G2|second period grade |\n",
    "|G3|final grade |\n",
    "\n",
    "## The Factor Analysis Model\n",
    "\n",
    "In the Factor Analysis model we assume a random sample $\\boldsymbol{y_1},\\boldsymbol{y_2}, . . . , \\boldsymbol{y_n}$ from a homogeneous population with mean vector $\\boldsymbol\\mu$ and covariance matrix $\\boldsymbol \\Sigma$.\n",
    "Each variable is expressed as a linear combination of underlying common factors $f_1, f_2, . . . , f_m$, with an error term to account for the unique part of the variable. For $y_1,y_2,...,y_p$ in any observation vector $\\boldsymbol y$,the model is:\n",
    "\n",
    "$$\n",
    "y_1 - \\mu_1 = \\lambda_{11}f_1 + \\lambda_{12}f_2 + ... + \\lambda_{1m}f_m + \\epsilon_1\n",
    "$$\n",
    "$$\n",
    "y_2 - \\mu_2 = \\lambda_{21}f_1 + \\lambda_{22}f_2 + ... + \\lambda_{2m}f_m + \\epsilon_2\n",
    "$$\n",
    "$$\n",
    "\\vdots\n",
    "$$\n",
    "$$\n",
    "y_p - \\mu_p = \\lambda_{p1}f_1 + \\lambda_{p2}f_2 + ... + \\lambda_{pm}f_m + \\epsilon_p\n",
    "$$\n",
    "\n",
    "The coefficients $\\lambda_{ij}$ are known as loadings and they serve as weights that show how each $y_i$ individually depends on the $f$’s.\n",
    "$\\lambda_{ij}$ indicates the importance of the $j$th factor $f_j$ to the $i$th variable $y_i$ and can be used to interpret of $f_j$. \n",
    "For example, we can interpret $f_1$ by examining its coefficients, $\\lambda_{11},\\lambda_{21},\\lambda_{p1}$. \n",
    "Larger loadings relate $f_1$ to the corresponding $y$’s. \n",
    "After estimating and rotating the $\\lambda_{ij}$’s we hope that they will split the variables into groups corresponding to factors.\n",
    "\n",
    "The Factor Analysis model expressed in matrix notation:\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix} \n",
    "y_1-\\mu_1 \\\\\n",
    "y_2-\\mu_2 \\\\\n",
    "\\vdots \\\\\n",
    "y_1-\\mu_1 \\\\\n",
    "\\end{pmatrix}\n",
    "= \n",
    "\\begin{pmatrix} \n",
    "\\lambda_{11} & \\lambda_{12} & \\dots & \\lambda_{1m}\\\\\n",
    "\\lambda_{21} & \\lambda_{22} & \\dots & \\lambda_{2m}\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "\\lambda_{p1} & \\lambda_{p2} & \\dots & \\lambda_{pm}\\\\\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix} \n",
    "f_{1} \\\\\n",
    "f_{2} \\\\\n",
    "\\vdots \\\\\n",
    "f_{m} \\\\\n",
    "\\end{pmatrix}\n",
    "+\n",
    "\\begin{pmatrix} \n",
    "\\epsilon_1 \\\\\n",
    "\\epsilon_2 \\\\\n",
    "\\vdots \\\\\n",
    "\\epsilon_p \\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "or equivalently:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{y} - \\boldsymbol{\\mu} = \\boldsymbol{\\Lambda f} + \\boldsymbol{\\epsilon}\n",
    "$$\n",
    "\n",
    "Assumptions:\n",
    "\n",
    "$$\n",
    "\\begin{gathered}\n",
    "E(\\boldsymbol{f}) = \\boldsymbol{0} \\\\ \\\\\n",
    "cov(\\boldsymbol{f}) = \\boldsymbol{I} \\\\ \\\\\n",
    "E(\\boldsymbol{\\epsilon}) = \\boldsymbol{0} \\\\ \\\\\n",
    "cov(\\boldsymbol\\epsilon) = \\boldsymbol\\Psi = \n",
    "\\begin{pmatrix}\n",
    "\\psi_1 & 0 & \\dots & 0 \\\\\n",
    "0 & \\psi_2 & \\dots & 0 \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "0 & \\dots & 0 & \\psi_p\n",
    "\\end{pmatrix} \\\\ \\\\\n",
    "cov(\\boldsymbol{f},\\boldsymbol{\\epsilon}) = \\boldsymbol{0} \\\\ \\\\\n",
    "\\boldsymbol{\\Sigma} = cov(\\boldsymbol{y}) = \n",
    "cov(\\boldsymbol{\\Lambda f} + \\boldsymbol{\\epsilon}) = \\boldsymbol{\\Lambda \\Lambda^\\prime} + \\boldsymbol{\\Psi} \\\\ \\\\\n",
    "cov(\\boldsymbol{y}, \\boldsymbol{f}) = \\boldsymbol{\\Lambda}\n",
    "\\end{gathered}\n",
    "$$\n",
    "\n",
    "Since we will be standardizing our variables, the loadings become correlations.\n",
    "\n",
    "$$\n",
    "corr(y_i,f_j) = \\lambda_{ij}\n",
    "$$\n",
    "\n",
    "The variance of $\\boldsymbol{y_i}$ is partitioned into 2 components, known as communality and specific variance. \n",
    "The communality component is from the common factors and the specific variance is unique to $\\boldsymbol{y_i}$. \n",
    "The communalities for the $i$th variable are computed by taking the sum of the squared loadings for that variable. \n",
    "\n",
    "1. Communality: $h_i^2 = \\lambda_{i1}^2 + \\lambda_{i2}^2 + ... + \\lambda_{im}^2$\n",
    "2. Specific Variance: $\\psi_i$\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "var(\\boldsymbol{y_i}) & = h_i^2 + \\psi_i \\\\\n",
    " & = (\\lambda_{i1}^2 + \\lambda_{i2}^2 + ... + \\lambda_{im}^2) + \\psi_i \n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "## Estimation of Loadings & Communalities\n",
    "\n",
    "### The Principal Component Method\n",
    "\n",
    "In this analysis of the student data, the Principal Component Method will be used to estimate the loadings and communalities. \n",
    "In this method, a sample covariance matrix **S** is obtained from the random sample $\\boldsymbol{y_1},\\boldsymbol{y_2}, . . . , \\boldsymbol{y_n}$.\n",
    "\n",
    "From the factor analysis model:\n",
    "$$\n",
    "\\boldsymbol{\\Sigma} = \\boldsymbol{\\Lambda \\Lambda^\\prime} + \\boldsymbol{\\Psi}\n",
    "$$\n",
    "The goal is to use the sample covariance matrix **S** in place of $\\boldsymbol \\Sigma$ to find an estimte for the factor loadings $\\boldsymbol{\\hat \\Lambda}$. \n",
    "$$\n",
    "\\boldsymbol{S} \\cong \\boldsymbol{\\hat\\Lambda} \\boldsymbol{\\hat\\Lambda^\\prime} + \\boldsymbol{\\Psi}\n",
    "$$\n",
    "This approach disregards $\\boldsymbol \\Psi$.\n",
    "$$\n",
    "\\boldsymbol{S} = \\boldsymbol{\\hat\\Lambda} \\boldsymbol{\\hat\\Lambda^\\prime} + {\\boldsymbol{\\Psi}}\n",
    "$$\n",
    "**S** is factored into the following:\n",
    "$$\n",
    "\\boldsymbol{S} = \\boldsymbol{\\hat\\Lambda} \\boldsymbol{\\hat\\Lambda^\\prime} = \\boldsymbol{CDC^\\prime} = (\\boldsymbol{CD^{1/2}}) (\\boldsymbol{CD^{1/2}})^\\prime\n",
    "$$\n",
    "\n",
    "- $\\boldsymbol{C}$: An orthogonal matrix constructed with normalized eigenvectors of **S**\n",
    "  \n",
    "- $\\boldsymbol{D}$: A diagonal matrix with the eigenvalues $\\theta_1, \\theta_2,..., \\theta_p$ on the main diagonal.\n",
    "\n",
    "Since the goal of factor analysis is to expresses each variable as a linear combination of $m$ underlying common factors $f_1, f_2, . . . , f_m$, we define:\n",
    "$$\n",
    "\\boldsymbol{C_1} = (\\boldsymbol{c_1}, \\boldsymbol{c_2},...,\\boldsymbol{c_m})\n",
    "$$\n",
    "$$\n",
    "\\boldsymbol{D_1} = diag(\\theta_1, \\theta_2,..., \\theta_m)\n",
    "$$\n",
    "The estimate of the the factor loadings matrix $\\boldsymbol{\\hat\\Lambda}$ then becomes the first $m$ columns of $\\boldsymbol{C_1}\\boldsymbol{D_1}^{1/2}$.\n",
    "$$\n",
    "\\boldsymbol{\\hat \\Lambda} = \\boldsymbol{C_1}\\boldsymbol{D_1}^{1/2} = \n",
    "(\\sqrt{\\theta_1}\\boldsymbol{c_1}, \\sqrt{\\theta_2}\\boldsymbol{c_2},..., \\sqrt{\\theta_m}\\boldsymbol{c_m})\n",
    "$$\n",
    "In Matrix Notation:\n",
    "$$\n",
    "\\begin{pmatrix} \n",
    "\\lambda_{11} & \\lambda_{12} & \\dots & \\lambda_{1m}\\\\\n",
    "\\lambda_{21} & \\lambda_{22} & \\dots & \\lambda_{2m}\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "\\lambda_{p1} & \\lambda_{p2} & \\dots & \\lambda_{pm}\\\\\n",
    "\\end{pmatrix}\n",
    "= \n",
    "\\begin{pmatrix} \n",
    "\\boldsymbol{c}_{11} & \\boldsymbol{c}_{12} & \\dots & \\boldsymbol{c}_{1m}\\\\\n",
    "\\boldsymbol{c}_{21} & \\boldsymbol{c}_{22} & \\dots & \\boldsymbol{c}_{2m}\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "\\boldsymbol{c}_{p1} & \\boldsymbol{c}_{p2} & \\dots & \\boldsymbol{c}_{pm}\\\\\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix} \n",
    "\\sqrt{\\theta_1} & 0 & \\dots & 0 \\\\\n",
    "0 & \\sqrt{\\theta_2} & \\dots & 0 \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "0 & 0 & \\dots & \\sqrt{\\theta_m}\n",
    "\\end{pmatrix}\n",
    "=\n",
    "\\begin{pmatrix} \n",
    "\\sqrt{\\theta_1}\\boldsymbol{c}_{11} & \\sqrt{\\theta_2}\\boldsymbol{c}_{12} & \\dots & \\sqrt{\\theta_m}\\boldsymbol{c}_{1m}\\\\\n",
    "\\sqrt{\\theta_1}\\boldsymbol{c}_{21} & \\sqrt{\\theta_2}\\boldsymbol{c}_{22} & \\dots & \\sqrt{\\theta_m}\\boldsymbol{c}_{2m}\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "\\sqrt{\\theta_1}\\boldsymbol{c}_{p1} & \\sqrt{\\theta_2}\\boldsymbol{c}_{p2} & \\dots & \\sqrt{\\theta_m}\\boldsymbol{c}_{pm}\\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "### Choosing the Number of Factors, $m$\n",
    "\n",
    "Particularly for the Principal Component Method, we choose $m$ equal to the number of factors necessary for the variance accounted for to achieve a certain percentage of the total variance tr(**R**). \n",
    "For this analysis, the number of factors will be the minimum number of factors needed to achieve 80% of the total variance tr(**R**). This is the proportion criterion.\n",
    "\n",
    "Since the emphasis in factor analysis is on reproducing the covariances or correlations rather than the variances, we will use the correlation matrix **R** to estimate the loadings instead of the covariance matrix **S**.\n",
    "\n",
    "![fact.png](https://github.com/BenitaDiop/students/blob/master/bin/fact.png?raw=true)\n",
    "\n",
    "\n",
    "\n",
    "## Interpretation of Factor Rotations\n",
    "\n",
    "In order to identify the groupings of variables, we can use a technique called a rotation. Rotation allows us to create an interpretable pattern for the loadings, in which the variables load highly on only one factor. \n",
    "The complexity of the variable is the number of factors on which a variable has moderate or high loadings.\n",
    "In this analysis we will use the *varimax* rotation.\n",
    "This technique aims to create rotated loadings that maximize the variance of the squared loadings.\n",
    "If all loadings in a given column were close in value, the variance would be near 0. \n",
    "The variance will approach a maximum as the squared loadings approach 0 and 1 (for factoring **R**).\n",
    "Therefore, the varimax method tries to make the loadings large or small for easier interpretation.\n",
    "\n",
    "For interpretability, the goal is to have a simple structure in the estimated loadings where each of the $p$ variable loads highly only on **one** of the $m$ factors. \n",
    "To assess the significance of these factor loadings, we do the following:\n",
    "\n",
    "1. Identify the loading with the greatest magnitude in each of the $p$ rows containing $m$ loadings in each row of the estimated loadings matrix.\n",
    "2. Use a specified **critical value**, which represents a threshold for the significance of the factor loadings.\n",
    "\n",
    "In this analysis, a target value of **0.4** is used. Table 5.4 and 5.5 are the Rotated Factor Patterns showing only significant loadings using the this critical value.\n",
    "From the **path diagram**,  Figure 5.3, we can visually see the which loadings are significant. \n",
    "Below are the groupings for Factor 1 and Factor 2.\n",
    "\n",
    "- Factor 1:\n",
    "\n",
    "    - Medu: Mother's Education\n",
    "    - Fedu: Father's Education\n",
    "    - failures: number of past class failures\n",
    "    - G1: first period grade\n",
    "    - G1: second period grade\n",
    "    - G3: final grade\n",
    "    \n",
    "- Factor 2:\n",
    "\n",
    "    - goout: going out with friends\n",
    "    - Dalc: weekend alcohol consumption\n",
    "    - Walc: workday alcohol consumption\n",
    "    \n",
    "The first factor seems to be attributed to variables relating to academics and the second factor seems to be attributed to alcohol and social activity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ref\"></a>\n",
    "# BIBLIOGRAPHY \n",
    "\n",
    "```\n",
    "P. Cortez and A. Silva. Using Data Mining to Predict Secondary School Student Performance. In A. Brito and J. Teixeira Eds., Proceedings of 5th FUture BUsiness TEChnology Conference (FUBUTEC 2008) pp. 5-12, Porto, Portugal, April, 2008, EUROSIS, ISBN 978-9077381-39-7.\n",
    "Available at: [Web Link]\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"add\"></a>\n",
    "# ADDENDUM\n",
    "\n",
    "![source.png](https://raw.githubusercontent.com/BenitaDiop/students/master/bin/source.png)\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md,Rmd"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
